<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Markov Chain Notes</title>
  <style>
    body { font-family: Arial, sans-serif; line-height: 1.6; margin: 20px; }
    h1, h2, h3 { color: #2C3E50; }
    pre { background: #f4f4f4; border: 1px solid #ccc; padding: 10px; overflow-x: auto; }
    code { background: #f4f4f4; padding: 2px 4px; }
    .section { margin-bottom: 30px; }
    .analogy { background: #e7f3fe; border-left: 4px solid #2196F3; padding: 10px; margin: 10px 0; }
    .example { background: #f9f9f9; border: 1px solid #ddd; padding: 10px; margin: 10px 0; }
    .photo { display: block; margin: 10px auto; max-width: 80%; }
  </style>
</head>
<body>
  <h1>Comprehensive Markov Chain Notes</h1>
  <p>This document provides detailed notes on Markov Chains, covering core concepts, analogies, examples, and key properties. It is designed to help you understand and solve problems using Markov chains.</p>

  <!-- Section: Markov Chain Concept -->
  <div class="section" id="concept">
    <h2>1. Markov Chain Concept</h2>
    <p>A Markov chain is a stochastic process that transitions from one state to another within a finite or countable set of states. The key characteristic of a Markov chain is that the next state depends only on the current state and not on the sequence of events that preceded it—this is known as the <strong>memoryless</strong> property.</p>
    <img src="https://imgs.search.brave.com/loF-_ha_AJaz0imejvB9xu4aceWfOozYbSDjON3laVk/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly91cGxv/YWQud2lraW1lZGlh/Lm9yZy93aWtpcGVk/aWEvY29tbW9ucy90/aHVtYi8yLzJiL01h/cmtvdmthdGVfMDEu/c3ZnLzY0MHB4LU1h/cmtvdmthdGVfMDEu/c3ZnLnBuZw" alt="Markov Chain Diagram" class="photo">
  </div>

  <!-- Section: Analogy -->
  <div class="section" id="analogy">
    <h2>2. Analogy</h2>
    <p>Imagine you are in a city with several bus stops, and at each bus stop you can catch a bus to a limited set of other stops. The bus you take only depends on the stop you're currently at and the schedule available there—your journey does not depend on how you arrived at that stop. In this analogy:</p>
    <ul>
      <li><strong>States</strong> are the bus stops.</li>
      <li><strong>Transition probabilities</strong> represent the likelihood of catching a bus from one stop to the next.</li>
      <li>The process is <strong>memoryless</strong> because the next stop only depends on your current stop.</li>
    </ul>
    <div class="analogy">
      <p>
        <strong>Analogy:</strong> "Imagine playing a board game where each move only depends on the current square you're on, not on all the previous moves you made."
      </p>
    </div>
  </div>

  <!-- Section: Components of a Markov Chain -->
  <div class="section" id="components">
    <h2>3. Components of a Markov Chain</h2>
    <h3>3.1 States</h3>
    <p>The state space is the set of all possible states the process can be in. States can be discrete (e.g., integers, names) and finite or countably infinite.</p>
    
    <h3>3.2 Transition Probabilities</h3>
    <p>Transition probabilities determine the likelihood of moving from one state to another. They are often represented in a <strong>transition matrix</strong> <code>P</code>, where each entry <code>P[i][j]</code> represents the probability of transitioning from state <code>i</code> to state <code>j</code>. The matrix rows sum to 1.</p>
    <div class="example">
      <pre>
# Example of a transition matrix for three states (A, B, and C)
P = [
  [0.2, 0.5, 0.3],  # Transitions from State A
  [0.1, 0.6, 0.3],  # Transitions from State B
  [0.4, 0.4, 0.2]   # Transitions from State C
]
      </pre>
    </div>
    
    <h3>3.3 Initial State Distribution</h3>
    <p>The initial state distribution indicates the probability of starting in each state. It is expressed as a probability vector that sums to 1.</p>
    <div class="example">
      <pre>
# Example initial distribution for states A, B, and C
initial_distribution = [0.5, 0.3, 0.2]
      </pre>
    </div>
    
    <h3>3.4 Stationary Distribution</h3>
    <p>A stationary distribution is a probability distribution that remains unchanged after transitions. It satisfies the equation <code>&pi;P = &pi;</code>, where <code>&pi;</code> is the stationary distribution vector. It is useful for long-term predictions.</p>
  </div>

  <!-- Section: Properties of Markov Chains -->
  <div class="section" id="properties">
    <h2>4. Key Properties of Markov Chains</h2>
    <ul>
      <li><strong>Memorylessness (Markov Property):</strong> Future states depend only on the present state.</li>
      <li><strong>Irreducibility:</strong> A Markov chain is irreducible if it is possible to reach any state from any other state.</li>
      <li><strong>Aperiodicity:</strong> A state is aperiodic if it can return to itself at irregular times, ensuring the chain does not get trapped in cyclic behavior.</li>
      <li><strong>Ergodicity:</strong> A chain is ergodic if it is both irreducible and aperiodic, meaning long-term behavior is independent of the starting state.</li>
      <li><strong>Absorbing States:</strong> States that, once entered, cannot be left.</li>
    </ul>
  </div>

  <!-- Section: Steps to Solve a Markov Chain Problem -->
  <div class="section" id="steps">
    <h2>5. Steps to Solve a Markov Chain Problem</h2>
    <ol>
      <li><strong>Define the State Space:</strong> Identify all possible states.</li>
      <li><strong>Establish Transition Probabilities:</strong> Construct the transition matrix <code>P</code> that outlines the probabilities of moving between states.</li>
      <li><strong>Determine the Initial State Distribution:</strong> Specify the starting probabilities for each state.</li>
      <li><strong>Analyze the Transition Behavior:</strong> Compute the state distribution after <code>n</code> steps using matrix multiplication (<code>initial_distribution * P^n</code>).</li>
      <li><strong>Find the Stationary Distribution (if applicable):</strong> Solve <code>&pi;P = &pi;</code> subject to the condition that the elements of <code>&pi;</code> sum to 1.</li>
      <li><strong>Interpret the Results:</strong> Use the computed probabilities to make predictions or decisions based on the Markov chain analysis.</li>
    </ol>
    <div class="example">
      <pre>
# Pseudo-code for computing state distributions after n steps:

def markov_chain_step(current_distribution, P):
    # Multiply current_distribution (vector) with transition matrix P
    return np.dot(current_distribution, P)

# Example using numpy for 3 steps:
import numpy as np

# Define the transition matrix P (rows sum to 1)
P = np.array([
    [0.2, 0.5, 0.3],
    [0.1, 0.6, 0.3],
    [0.4, 0.4, 0.2]
])

# Define the initial distribution (e.g., state A: 50%, B: 30%, C: 20%)
initial_distribution = np.array([0.5, 0.3, 0.2])

# Compute the distribution after 3 steps
n = 3
distribution_after_n = np.linalg.matrix_power(P, n).dot(initial_distribution)
print("Distribution after 3 steps:", distribution_after_n)
      </pre>
    </div>
  </div>

  <!-- Section: Example Problem -->
  <div class="section" id="example-problem">
    <h2>6. Example Markov Chain Problem</h2>
    <p>Suppose we have a weather system modeled as a Markov chain with the states: <em>Sunny</em> and <em>Rainy</em>. The transition matrix is given by:</p>
    <div class="example">
      <pre>
P = [
  [0.8, 0.2],  # Sunny to Sunny, Sunny to Rainy
  [0.4, 0.6]   # Rainy to Sunny, Rainy to Rainy
]
      </pre>
    </div>
    <p>If the initial state distribution is <code>[0.6, 0.4]</code> (60% chance Sunny, 40% chance Rainy), we can analyze the weather pattern over the next few days. Calculate the distribution after 2 days and discuss if a stationary distribution exists.</p>
    <p><strong>Steps:</strong></p>
    <ul>
      <li>Determine the initial state distribution: <code>[0.6, 0.4]</code>.</li>
      <li>Compute <code>P^2</code> (transition probabilities over 2 days) and multiply by the initial distribution.</li>
      <li>Check if the state distribution converges to a stationary distribution as <code>n</code> increases.</li>
    </ul>
    <img src="https://via.placeholder.com/600x300?text=Weather+Markov+Chain" alt="Weather Markov Chain Example" class="photo">
  </div>

  <!-- Section: Conclusion -->
  <div class="section" id="conclusion">
    <h2>Conclusion</h2>
    <p>Markov chains provide a powerful framework for modeling systems where the future state depends solely on the current state. By understanding the components—states, transition probabilities, initial and stationary distributions—and following a systematic approach to solve problems, you can apply Markov chains to various real-world scenarios such as weather prediction, board games, queueing systems, and more.</p>
    <p>Practice by setting up your own Markov chain models and exploring their long-term behavior using simulation or matrix computations.</p>
  </div>
</body>
</html>
